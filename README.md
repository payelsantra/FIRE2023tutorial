# Unleashing the Power of Large Language Models: A Hands-On Tutorial
This repository contains all detailed information and resources for our tutorial at FIRE 2023, held at the University of Goa, India (Dec 2023).

# Abstract
Large Language Models (LLMs) have exhibited exceptional proficiency in the realm of natural language processing and other domains. The effectiveness of LLMs has led to a steep rise in various research fields including both academia and industry. In this tutorial, we present the audience with an introduction to LLMs and the associated challenges. We briefly discuss certain recent publicly accessible LLMs, specifically a robust chatbot named ChatGPT, and the significant impact that the technological advancement of LLMs has had on the entire AI community. The tutorial is structured in the following manner. First, we provide  a brief preface that outlines the fundamental principles of Natural Language Processing (NLP). Following that, we explore the area of distributional representation learning for NLP. Later on, we delve into the essential component of transformer-based pretrained language models. We gradually outline the concept of prompt learning and how it replaced the pretraining and finetuning process. Then we discuss the concept of in-context learning (ICL) in LLMs, which entails making predictions based on augmented contexts with a limited number of examples and also the challenges of using LLMs. Afterward, a hands-on coding and demonstration session is carried out in a practical manner.

# About the tutorial


# Authors

* [Payel Santra], IACS, Kolkata, India
* [Madhusudan Ghosh], IACS, Kolkata, India
* [Shrimon Mukherjee], IACS, Kolkata, India
* [Debasis Ganguly](https://gdebasis.github.io/), University of Glasgow, UK
* [Partha Basuchowdhuri](http://iacs.res.in/athusers/index.php?navid=0&userid=IACS0043), IACS, Kolkata, India
* [Sudip Kumar Naskar](https://sites.google.com/site/sudipnaskar/), Jadavpur University, India

# Tutorial Outline
**Part** | **Topic** | **Presenter** | **Link to Slides**
--- | --- | --- | ---
1 | Introduction to NLP | Saptarshi Ghosh | [Slides](https://github.com/Law-AI/ecir2023tutorial/blob/main/legal-text-background.pdf)
2 | Overview of Distributional Representation Learning for NLP | Jack G. Conrad | [Slides](https://github.com/Law-AI/ecir2023tutorial/blob/main/history-ai-law.pdf)
3 | Overview of Transformer based pretrained
Language Model | Pawan Goyal | [Slides](https://github.com/Law-AI/ecir2023tutorial/blob/main/nlp-ir-background.pdf)
4 | Overview of Large Language Models | Debasis Ganguly, Paheli Bhattacharya and Kripabandhu Ghosh | [Slides](https://github.com/Law-AI/ecir2023tutorial/blob/main/sota-survey.pdf)
5 | Concept of in-context learning and its application | Jack G. Conrad | [Slides](https://github.com/Law-AI/ecir2023tutorial/blob/main/industry-pov.pdf)
6 | Future directions | Jack G. Conrad, Kripabandhu Ghosh and Saptarshi Ghosh | [Slides](https://github.com/Law-AI/ecir2023tutorial/blob/main/future-directions.pdf)
7 | Hands-on Coding/Demo Session | Debasis Ganguly, Paheli Bhattacharya, Shounak Paul and Shubham Kumar Nigam | [JuPyter Notebook](https://github.com/Law-AI/ecir2023tutorial/blob/main/hands-on.ipynb)


# Useful Links


# Citation Policy
If you make using of any of these slides, notebooks, or additional PyTerrier plugins, please cite our tutorial abstract: 

# Feedback
